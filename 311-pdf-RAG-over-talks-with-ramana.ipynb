{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Twitter the-algorithm source code with LangChain, GPT4 and Deep Lake\n",
    "In this tutorial, we are going to use Langchain + Deep Lake with GPT4 to analyze the code base of the twitter algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate. For full documentation of Deep Lake please follow [docs](https://docs.activeloop.ai/) and [API reference](https://docs.deeplake.ai/en/latest/).\n",
    "\n",
    "Authenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the [platform](https://app.activeloop.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_CACHE=\"/f/C/cache/huggingface/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm as notebook_tqdm\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'F:\\C\\cache\\huggingface\\hub'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'xxxxxxxxxx.xxxxxxxxxx.FeYGCK2Cc-xxxxxxxxxxxxxxx'\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files inside the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import tiktoken\n",
    "\n",
    "root_dir = '../dev/langchain/langchain'  # Declare root directory variable\n",
    "docs = []  # Create empty list for documents\n",
    "\n",
    "pdf_loader = PyPDFLoader('../sources-docs/Talks-with-Sri-Ramana-Maharshi--complete.pdf')  # Use PDFLoader to load and split the PDF file's content\n",
    "docs.extend(pdf_loader.load()) # just load, don't split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='31Talks with Sri Ramana Maharshi\\n(a) Is omniscience of God consistent with ego’s freewill?\\n(b) Is omnipotence of God consistent with ego’s freewill?\\n(c) Are the natural laws consistent with God’s free-will?\\nM.: Yes. Free-will is the present appearing to a limited faculty of \\nsight and will. The same ego sees its past activity as falling into a \\ncourse of ‘law’ or rules - its own free-will being one of the links \\nin that course of law.\\nOmnipotence and omniscience of God are then seen by the ego to \\nhave acted through the appearance of his own free-will. So he comes \\nto the conclusion that the ego must go by appearances. Natural laws \\nare manifestations of God’s will and they have been laid down.\\nD.: Is the study of science, psychology, physiology, philosophy, etc. \\nhelpful for:-\\n(1) this art of yoga-liberation.\\n(2) the intuitive grasp of the unity of the Real?\\nM.: Very little. Some knowledge is needed for yoga and it may be \\nfound in books. But practical application is the thing needed, and \\npersonal example, personal touch and personal instructions are the \\nmost helpful aids. As for the other, a person may laboriously convince \\nhimself of the truth to be intuited, i.e., its function and nature, but the \\nactual intuition is akin to feeling and requires practice and personal \\ncontact. Mere book learning is not of any great use. After realisation \\nall intellectual loads are useless burdens and are thrown overboard \\nas jetsam. Jettisoning the ego is necessary and natural.\\nD.: How does dream differ from waking?\\nM.: In dreams one takes on different bodies, and they re-enter this \\nbody when one dreams of sense-contacts.\\nD.: What is happiness? Is it inhering in the Atman or in the object, or \\nin the contact between the subject and the object? But we do not \\nsee happiness in our affairs. When does It actually arise?\\nM.: When there is contact of a desirable sort or memory thereof, \\nand when there is freedom from undesirable contacts or memory \\nthereof, we say there is happiness. Such happiness is relative and \\nis better called pleasure.' metadata={'source': '../Talks-with-Sri-Ramana-Maharshi--complete.pdf', 'page': 40}\n",
      "<class 'langchain.schema.Document'>\n",
      "page_content='(a) Is omniscience of God consistent with ego’s freewill?\\n(b) Is omnipotence of God consistent with ego’s freewill?\\n(c) Are the natural laws consistent with God’s free-will?\\nM.: Yes. Free-will is the present appearing to a limited faculty of \\nsight and will. The same ego sees its past activity as falling into a \\ncourse of ‘law’ or rules - its own free-will being one of the links \\nin that course of law.\\nOmnipotence and omniscience of God are then seen by the ego to \\nhave acted through the appearance of his own free-will. So he comes \\nto the conclusion that the ego must go by appearances. Natural laws \\nare manifestations of God’s will and they have been laid down.\\nD.: Is the study of science, psychology, physiology, philosophy, etc. \\nhelpful for:-\\n(1) this art of yoga-liberation.\\n(2) the intuitive grasp of the unity of the Real?\\nM.: Very little. Some knowledge is needed for yoga and it may be \\nfound in books. But practical application is the thing needed, and \\npersonal example, personal touch and personal instructions are the \\nmost helpful aids. As for the other, a person may laboriously convince \\nhimself of the truth to be intuited, i.e., its function and nature, but the \\nactual intuition is akin to feeling and requires practice and personal \\ncontact. Mere book learning is not of any great use. After realisation \\nall intellectual loads are useless burdens and are thrown overboard \\nas jetsam. Jettisoning the ego is necessary and natural.\\nD.: How does dream differ from waking?\\nM.: In dreams one takes on different bodies, and they re-enter this \\nbody when one dreams of sense-contacts.\\nD.: What is happiness? Is it inhering in the Atman or in the object, or \\nin the contact between the subject and the object? But we do not \\nsee happiness in our affairs. When does It actually arise?\\nM.: When there is contact of a desirable sort or memory thereof, \\nand when there is freedom from undesirable contacts or memory \\nthereof, we say there is happiness. Such happiness is relative and \\nis better called pleasure.' metadata={'source': '../Talks-with-Sri-Ramana-Maharshi--complete.pdf', 'page': 40}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "len(docs) # number of processed document pages\n",
    "print(docs[40]) # returns 31Talks with Sri Ramana Maharshi\\n(a) Is omniscience of God consis ....\n",
    "print(type(docs[40])) # <class 'str'>\n",
    "# Use regex to remove \"31Talks with Sri Ramana Maharshi\\n\"\n",
    "pg_40 = re.sub(r'\\d{1,3}Talks with Sri Ramana Maharshi\\\\n', '', str(docs[40]))\n",
    "print(pg_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Token Length in Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Avg: 490\n",
      "Max: 812\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_counts = [tiktoken_len(doc.page_content) for doc in docs]\n",
    "\n",
    "print(f\"\"\"Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\"\"\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set style and color palette for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(token_counts, kde=False, bins=50)\n",
    "\n",
    "# customize the plot info\n",
    "plt.title(\"Token Counts Histogram\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, chunk the files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='16Talks with Sri Ramana Maharshi\\nD.: They say that there are many saints in Tibet who remain in solitude \\nand are still very helpful to the world. How can it be?\\nM.: It can be so. Realisation of the Self is the greatest help that can be \\nrendered to humanity. Therefore, the saints are said to be helpful, \\nthough they remain in forests. But it should not be forgotten that \\nsolitude is not in forests only. It can be had even in towns, in the \\nthick of worldly occupations.\\nD.: It is not necessary that the saints should mix with people and be \\nhelpful to them?\\nM.: The Self alone is the Reality; the world and the rest of it are not. The \\nrealised being does not see the world as different from himself.\\nD.: Thus then, the saint’s realisation leads to the uplift of humanity \\nwithout the latter being aware of it. Is it so?\\nM.: Yes. The help is imperceptible but is still there. A saint helps the \\nwhole of humanity, unknown to the latter.\\nD.: Would it not be better if he mixed with others?\\nM.: There are no others to mix with. The Self is the one and only \\nReality.\\nD.: If there be a hundred Self-realised men will it not be to the greater \\nbenefit of the world?\\nM.: When you say ‘Self’ you refer to the unlimited, but when you add \\n‘men’ to it, you limit the meaning. There is only one Infinite Self.\\nD.: Yes, yes, I see! Sri Krishna has said in the Gita that work must \\nbe performed without attachment and such work is better than \\nidleness. Is it Karma Yoga?\\nM.: What is said is given out to suit the temperament of the hearers.\\nD.: In Europe it is not understood by the people that a man in solitude \\ncan be helpful. They imagine that men working in the world can \\nalone be useful. When will this confusion cease? Will the European \\nmind continue wading in the morass or will it realise the truth?\\nM.: Never mind Europe or America. Where are they except in your \\nmind? Realise your Self and then all is realised.\\nIf you dream and see several men, and then wake up and recall \\nyour dream, do you try to ascertain if the persons of your dream \\ncreation are also awake?' metadata={'source': '../Talks-with-Sri-Ramana-Maharshi--complete.pdf', 'page': 25}\n"
     ]
    }
   ],
   "source": [
    "print(docs[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5685943fa1344e30b1a7bd6847501cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hashlib\n",
    "m = hashlib.md5()  # this will convert URL into unique ID\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.docstore.document import Document\n",
    "documents = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    url = doc.metadata['source'].replace('../dev/langchain/langchain\\\\', 'https://github.com/hwchase17/langchain/tree/master/langchain/')\n",
    "    m.update(url.encode('utf-8'))\n",
    "    uid = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content = chunk\n",
    "            )\n",
    "        )\n",
    "            # {'text': chunk})\n",
    "        # documents.append({\n",
    "        #     'id': f'{uid}-{i}',\n",
    "        #     'text': chunk,\n",
    "        #     'source': url\n",
    "        # })\n",
    "        # documents.extend(f\"id={uid}-{i}\")\n",
    "        # documents.extend('text={chunk}')\n",
    "        # documents.extend('source={url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='16Talks with Sri Ramana Maharshi\\nD.: They say that there are many saints in Tibet who remain in solitude \\nand are still very helpful to the world. How can it be?\\nM.: It can be so. Realisation of the Self is the greatest help that can be \\nrendered to humanity. Therefore, the saints are said to be helpful, \\nthough they remain in forests. But it should not be forgotten that \\nsolitude is not in forests only. It can be had even in towns, in the \\nthick of worldly occupations.\\nD.: It is not necessary that the saints should mix with people and be \\nhelpful to them?\\nM.: The Self alone is the Reality; the world and the rest of it are not. The \\nrealised being does not see the world as different from himself.\\nD.: Thus then, the saint’s realisation leads to the uplift of humanity \\nwithout the latter being aware of it. Is it so?\\nM.: Yes. The help is imperceptible but is still there. A saint helps the \\nwhole of humanity, unknown to the latter.\\nD.: Would it not be better if he mixed with others?\\nM.: There are no others to mix with. The Self is the one and only \\nReality.\\nD.: If there be a hundred Self-realised men will it not be to the greater \\nbenefit of the world?\\nM.: When you say ‘Self’ you refer to the unlimited, but when you add \\n‘men’ to it, you limit the meaning. There is only one Infinite Self.\\nD.: Yes, yes, I see! Sri Krishna has said in the Gita that work must \\nbe performed without attachment and such work is better than \\nidleness. Is it Karma Yoga?\\nM.: What is said is given out to suit the temperament of the hearers.\\nD.: In Europe it is not understood by the people that a man in solitude \\ncan be helpful. They imagine that men working in the world can \\nalone be useful. When will this confusion cease? Will the European \\nmind continue wading in the morass or will it realise the truth?\\nM.: Never mind Europe or America. Where are they except in your \\nmind? Realise your Self and then all is realised.\\nIf you dream and see several men, and then wake up and recall \\nyour dream, do you try to ascertain if the persons of your dream \\ncreation are also awake?' metadata={'source': '../Talks-with-Sri-Ramana-Maharshi--complete.pdf', 'page': 25}\n"
     ]
    }
   ],
   "source": [
    "len(documents)\n",
    "print(docs[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\"\"\"Beta Feature: base interface for cache.\"\"\"\\nimport json\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\\n\\nfrom sqlalchemy import Column, Integer, String, create_engine, select\\nfrom sqlalchemy.engine.base import Engine\\nfrom sqlalchemy.orm import Session\\n\\ntry:\\n    from sqlalchemy.orm import declarative_base\\nexcept ImportError:\\n    from sqlalchemy.ext.declarative import declarative_base\\n\\nfrom langchain.schema import Generation\\n\\nRETURN_VAL_TYPE = List[Generation]\\n\\n\\nclass BaseCache(ABC):\\n    \"\"\"Base interface for cache.\"\"\"\\n\\n    @abstractmethod\\n    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:\\n        \"\"\"Look up based on prompt and llm_string.\"\"\"\\n\\n    @abstractmethod\\n    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:\\n        \"\"\"Update cache based on prompt and llm_string.\"\"\"\\n\\n\\nclass InMemoryCache(BaseCache):\\n    \"\"\"Cache that stores things in memory.\"\"\"' metadata={'source': '../dev/langchain/langchain\\\\cache.py'}\n",
      "<class 'langchain.schema.Document'>\n",
      "page_content='TALKS \\nWITH \\nSRI RAMANA  MAHARSHI\\nSRI RAMANASRAMAM\\nTiruvannamalai\\n2006' metadata={'id': 'cb9055527713-0', 'source': '../Talks-with-Sri-Ramana-Maharshi--complete.pdf'}\n",
      "<class 'langchain.schema.Document'>\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the indexing. This will take about ~4 mins to compute embeddings and upload to Activeloop. You can then publish the dataset to be public."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"ramana_talks2\"\n",
    "\n",
    "content_source = documents\n",
    "# content_source = text   # Old chunking method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem://ramana_talks loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 0%|          | 0/2 [00:00<?"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m db \u001b[39m=\u001b[39m DeepLake\u001b[39m.\u001b[39;49mfrom_documents(content_source, embeddings, dataset_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmem://\u001b[39;49m\u001b[39m{\u001b[39;49;00mdatabase_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\base.py:218\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    217\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 218\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:476\u001b[0m, in \u001b[0;36mDeepLake.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, dataset_path, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Deep Lake dataset from a raw documents.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[0;32m    446\u001b[0m \u001b[39mIf a dataset_path is specified, the dataset will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39m    DeepLake: Deep Lake dataset.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    473\u001b[0m deeplake_dataset \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    474\u001b[0m     dataset_path\u001b[39m=\u001b[39mdataset_path, embedding_function\u001b[39m=\u001b[39membedding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    475\u001b[0m )\n\u001b[1;32m--> 476\u001b[0m deeplake_dataset\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[0;32m    477\u001b[0m \u001b[39mreturn\u001b[39;00m deeplake_dataset\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:222\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mingestion_batch_size, \u001b[39mlen\u001b[39m(elements))\n\u001b[0;32m    218\u001b[0m batched \u001b[39m=\u001b[39m [\n\u001b[0;32m    219\u001b[0m     elements[i : i \u001b[39m+\u001b[39m batch_size] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(elements), batch_size)\n\u001b[0;32m    220\u001b[0m ]\n\u001b[1;32m--> 222\u001b[0m ingest()\u001b[39m.\u001b[39;49meval(\n\u001b[0;32m    223\u001b[0m     batched,\n\u001b[0;32m    224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds,\n\u001b[0;32m    225\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39mmin\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers, \u001b[39mlen\u001b[39;49m(batched) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers),\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39mcommit(allow_empty\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\transform\\transform.py:95\u001b[0m, in \u001b[0;36mComputeFunction.eval\u001b[1;34m(self, data_in, ds_out, num_workers, scheduler, progressbar, skip_ok, check_lengths, pad_data_in, read_only_ok, checkpoint_interval, ignore_errors, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluates the ComputeFunction on data_in to produce an output dataset ds_out.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39m    ValueError: If ``num_workers`` > 0 and ``checkpoint_interval`` is not a multiple of ``num_workers`` or if ``checkpoint_interval`` > 0 and ds_out is None.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline([\u001b[39mself\u001b[39m])\n\u001b[1;32m---> 95\u001b[0m pipeline\u001b[39m.\u001b[39meval(\n\u001b[0;32m     96\u001b[0m     data_in,\n\u001b[0;32m     97\u001b[0m     ds_out,\n\u001b[0;32m     98\u001b[0m     num_workers,\n\u001b[0;32m     99\u001b[0m     scheduler,\n\u001b[0;32m    100\u001b[0m     progressbar,\n\u001b[0;32m    101\u001b[0m     skip_ok,\n\u001b[0;32m    102\u001b[0m     check_lengths,\n\u001b[0;32m    103\u001b[0m     pad_data_in,\n\u001b[0;32m    104\u001b[0m     read_only_ok,\n\u001b[0;32m    105\u001b[0m     checkpoint_interval,\n\u001b[0;32m    106\u001b[0m     ignore_errors,\n\u001b[0;32m    107\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    108\u001b[0m )\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\transform\\transform.py:260\u001b[0m, in \u001b[0;36mPipeline.eval\u001b[1;34m(self, data_in, ds_out, num_workers, scheduler, progressbar, skip_ok, check_lengths, pad_data_in, read_only_ok, checkpoint_interval, ignore_errors, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m progress_args \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcompute_id\u001b[39m\u001b[39m\"\u001b[39m: compute_id, \u001b[39m\"\u001b[39m\u001b[39mprogress\u001b[39m\u001b[39m\"\u001b[39m: progress, \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m: end}\n\u001b[0;32m    259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun(\n\u001b[0;32m    261\u001b[0m         data_in,\n\u001b[0;32m    262\u001b[0m         target_ds,\n\u001b[0;32m    263\u001b[0m         compute_provider,\n\u001b[0;32m    264\u001b[0m         num_workers,\n\u001b[0;32m    265\u001b[0m         scheduler,\n\u001b[0;32m    266\u001b[0m         progressbar,\n\u001b[0;32m    267\u001b[0m         overwrite,\n\u001b[0;32m    268\u001b[0m         skip_ok,\n\u001b[0;32m    269\u001b[0m         read_only_ok \u001b[39mand\u001b[39;00m overwrite,\n\u001b[0;32m    270\u001b[0m         pbar,\n\u001b[0;32m    271\u001b[0m         pqueue,\n\u001b[0;32m    272\u001b[0m         ignore_errors,\n\u001b[0;32m    273\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    275\u001b[0m     target_ds\u001b[39m.\u001b[39m_send_compute_progress(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprogress_args, status\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m     samples_processed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_in)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\transform\\transform.py:399\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, data_in, target_ds, compute, num_workers, scheduler, progressbar, overwrite, skip_ok, read_only, pbar, pqueue, ignore_errors, **kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mif\u001b[39;00m progressbar:\n\u001b[0;32m    398\u001b[0m     desc \u001b[39m=\u001b[39m get_pbar_description(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctions)\n\u001b[1;32m--> 399\u001b[0m     result \u001b[39m=\u001b[39m compute\u001b[39m.\u001b[39;49mmap_with_progress_bar(\n\u001b[0;32m    400\u001b[0m         store_data_slice_with_pbar,\n\u001b[0;32m    401\u001b[0m         map_inp,\n\u001b[0;32m    402\u001b[0m         total_length\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(data_in),\n\u001b[0;32m    403\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    404\u001b[0m         pbar\u001b[39m=\u001b[39;49mpbar,\n\u001b[0;32m    405\u001b[0m         pqueue\u001b[39m=\u001b[39;49mpqueue,\n\u001b[0;32m    406\u001b[0m     )\n\u001b[0;32m    407\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     result \u001b[39m=\u001b[39m compute\u001b[39m.\u001b[39mmap(store_data_slice, map_inp)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\compute\\serial.py:28\u001b[0m, in \u001b[0;36mSerialProvider.map_with_progress_bar\u001b[1;34m(self, func, iterable, total_length, desc, pbar, pqueue)\u001b[0m\n\u001b[0;32m     24\u001b[0m         progress_bar\u001b[39m.\u001b[39mupdate(value)\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m func(pg_callback, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 28\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap(sub_func, iterable)\n\u001b[0;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\compute\\serial.py:9\u001b[0m, in \u001b[0;36mSerialProvider.map\u001b[1;34m(self, func, iterable)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable):\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(func, iterable))\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\core\\compute\\serial.py:26\u001b[0m, in \u001b[0;36mSerialProvider.map_with_progress_bar.<locals>.sub_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpg_callback\u001b[39m(value: \u001b[39mint\u001b[39m):\n\u001b[0;32m     24\u001b[0m     progress_bar\u001b[39m.\u001b[39mupdate(value)\n\u001b[1;32m---> 26\u001b[0m \u001b[39mreturn\u001b[39;00m func(pg_callback, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\util\\transform.py:157\u001b[0m, in \u001b[0;36mstore_data_slice_with_pbar\u001b[1;34m(pg_callback, transform_input)\u001b[0m\n\u001b[0;32m    148\u001b[0m     extend_data_slice(\n\u001b[0;32m    149\u001b[0m         data_slice,\n\u001b[0;32m    150\u001b[0m         offset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m         pg_callback,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     transform_data_slice_and_append(\n\u001b[0;32m    158\u001b[0m         data_slice,\n\u001b[0;32m    159\u001b[0m         offset,\n\u001b[0;32m    160\u001b[0m         pipeline,\n\u001b[0;32m    161\u001b[0m         visible_tensors,\n\u001b[0;32m    162\u001b[0m         label_temp_tensors,\n\u001b[0;32m    163\u001b[0m         actual_tensors,\n\u001b[0;32m    164\u001b[0m         all_chunk_engines,\n\u001b[0;32m    165\u001b[0m         group_index,\n\u001b[0;32m    166\u001b[0m         pg_callback,\n\u001b[0;32m    167\u001b[0m         skip_ok,\n\u001b[0;32m    168\u001b[0m         ignore_errors,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    171\u001b[0m \u001b[39m# retrieve relevant objects from memory\u001b[39;00m\n\u001b[0;32m    172\u001b[0m all_tensor_metas \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\util\\transform.py:328\u001b[0m, in \u001b[0;36mtransform_data_slice_and_append\u001b[1;34m(data_slice, offset, pipeline, tensors, label_temp_tensors, actual_tensors, all_chunk_engines, group_index, pg_callback, skip_ok, ignore_errors)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m i, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[0;32m    323\u001b[0m     (data_slice[i : i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n))\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m pd \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data_slice, pd\u001b[39m.\u001b[39mDataFrame)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m data_slice\n\u001b[0;32m    326\u001b[0m ):\n\u001b[0;32m    327\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m         _transform_sample_and_update_chunk_engines(\n\u001b[0;32m    329\u001b[0m             sample,\n\u001b[0;32m    330\u001b[0m             pipeline,\n\u001b[0;32m    331\u001b[0m             tensors,\n\u001b[0;32m    332\u001b[0m             label_temp_tensors,\n\u001b[0;32m    333\u001b[0m             actual_tensors,\n\u001b[0;32m    334\u001b[0m             all_chunk_engines,\n\u001b[0;32m    335\u001b[0m             group_index,\n\u001b[0;32m    336\u001b[0m             skip_ok,\n\u001b[0;32m    337\u001b[0m         )\n\u001b[0;32m    338\u001b[0m         \u001b[39mif\u001b[39;00m pg_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m             curr_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\util\\transform.py:218\u001b[0m, in \u001b[0;36m_transform_sample_and_update_chunk_engines\u001b[1;34m(sample, pipeline, tensors, label_temp_tensors, actual_tensors, all_chunk_engines, group_index, skip_ok)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform_sample_and_update_chunk_engines\u001b[39m(\n\u001b[0;32m    209\u001b[0m     sample,\n\u001b[0;32m    210\u001b[0m     pipeline,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     skip_ok: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    217\u001b[0m ):\n\u001b[1;32m--> 218\u001b[0m     result \u001b[39m=\u001b[39m transform_sample(sample, pipeline)\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m is_empty_transform_dataset(result):\n\u001b[0;32m    220\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\deeplake\\util\\transform.py:85\u001b[0m, in \u001b[0;36mtransform_sample\u001b[1;34m(sample, pipeline)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     samples_out \u001b[39m=\u001b[39m TransformDataset()\n\u001b[1;32m---> 85\u001b[0m     fn(result, samples_out, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     86\u001b[0m     validate_transform_dataset(samples_out)\n\u001b[0;32m     87\u001b[0m     result \u001b[39m=\u001b[39m samples_out\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:202\u001b[0m, in \u001b[0;36mDeepLake.add_texts.<locals>.ingest\u001b[1;34m(sample_in, sample_out)\u001b[0m\n\u001b[0;32m    199\u001b[0m embeds: Sequence[Optional[np\u001b[39m.\u001b[39mndarray]] \u001b[39m=\u001b[39m []\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(text_list)\n\u001b[0;32m    203\u001b[0m     embeds \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39marray(e, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m embeddings]\n\u001b[0;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\embeddings\\huggingface.py:71\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m), texts))\n\u001b[1;32m---> 71\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mencode(texts)\n\u001b[0;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:188\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[39m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[0;32m    187\u001b[0m             \u001b[39mif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m--> 188\u001b[0m                 embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    190\u001b[0m         all_embeddings\u001b[39m.\u001b[39mextend(embeddings)\n\u001b[0;32m    192\u001b[0m all_embeddings \u001b[39m=\u001b[39m [all_embeddings[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "db = DeepLake.from_documents(content_source, embeddings, dataset_path=f\"mem://{database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem://ramana_talks loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=f\"mem://{database_name}\", read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote storage\n",
    "\n",
    "Uses Activeloop API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ACTIVELOOP_TOKEN'] = 'xxxxxxxxxxxxx.xxxx.xxxxxxxxxxxxx'\n",
    "db = DeepLake.from_documents(content_source, embeddings, dataset_path=f\"hub://deadbranches/{database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(dataset_path=f\"hub://deadbranches/{database_name}\", read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Question Answering on Twitter algorithm codebase\n",
    "First load the dataset, construct the retriever, then construct the Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'dot'\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (768,) and (0,) not aligned: 768 (dim 0) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m Document\n\u001b[1;32m----> 4\u001b[0m results \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49msimilarity_search(\u001b[39m'\u001b[39;49m\u001b[39mWhat did the president say about Ketanji Brown Jackson?\u001b[39;49m\u001b[39m'\u001b[39;49m, distance_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdot\u001b[39;49m\u001b[39m'\u001b[39;49m, k\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclass:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(results)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:350\u001b[0m, in \u001b[0;36mDeepLake.similarity_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    324\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[0;32m    327\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m        List of Documents most similar to the query vector.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch(query\u001b[39m=\u001b[39mquery, k\u001b[39m=\u001b[39mk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:294\u001b[0m, in \u001b[0;36mDeepLake.search\u001b[1;34m(self, query, embedding, k, distance_metric, use_maximal_marginal_relevance, fetch_k, filter, return_score, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m embeddings \u001b[39m=\u001b[39m view\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mnumpy(fetch_chunks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m k_search \u001b[39m=\u001b[39m fetch_k \u001b[39mif\u001b[39;00m use_maximal_marginal_relevance \u001b[39melse\u001b[39;00m k\n\u001b[1;32m--> 294\u001b[0m indices, scores \u001b[39m=\u001b[39m vector_search(\n\u001b[0;32m    295\u001b[0m     query_emb,\n\u001b[0;32m    296\u001b[0m     embeddings,\n\u001b[0;32m    297\u001b[0m     k\u001b[39m=\u001b[39;49mk_search,\n\u001b[0;32m    298\u001b[0m     distance_metric\u001b[39m=\u001b[39;49mdistance_metric\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    299\u001b[0m )\n\u001b[0;32m    301\u001b[0m view \u001b[39m=\u001b[39m view[indices]\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m use_maximal_marginal_relevance:\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:47\u001b[0m, in \u001b[0;36mvector_search\u001b[1;34m(query_embedding, data_vectors, distance_metric, k)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Naive search for nearest neighbors\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m    nearest_indices: List, indices of nearest neighbors\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m# Calculate the distance between the query_vector and all data_vectors\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m distances \u001b[39m=\u001b[39m distance_metric_map[distance_metric](query_embedding, data_vectors)\n\u001b[0;32m     48\u001b[0m nearest_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(distances)\n\u001b[0;32m     50\u001b[0m nearest_indices \u001b[39m=\u001b[39m (\n\u001b[0;32m     51\u001b[0m     nearest_indices[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:k] \u001b[39mif\u001b[39;00m distance_metric \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcos\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m nearest_indices[:k]\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m maximal_marginal_relevance\n\u001b[0;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     18\u001b[0m distance_metric_map \u001b[39m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39minf),\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcos\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mdot(a, b\u001b[39m.\u001b[39mT)\n\u001b[0;32m     23\u001b[0m     \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m---> 24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39;49mdot(a, b\u001b[39m.\u001b[39;49mT),\n\u001b[0;32m     25\u001b[0m }\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvector_search\u001b[39m(\n\u001b[0;32m     29\u001b[0m     query_embedding: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     30\u001b[0m     data_vectors: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     31\u001b[0m     distance_metric: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     k: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[0;32m     33\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List, List]:\n\u001b[0;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Naive search for nearest neighbors\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39m    args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m        nearest_indices: List, indices of nearest neighbors\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (768,) and (0,) not aligned: 768 (dim 0) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "results = db.similarity_search('What did the president say about Ketanji Brown Jackson?', distance_metric='dot', k=4)\n",
    "\n",
    "print(\"class:\", type(results).__name__)\n",
    "print(\"[\")\n",
    "for result in results:\n",
    "    print(\"    {\")\n",
    "    print(f\"        'page_content': {json.dumps(result.page_content)},\")\n",
    "    print(f\"        'metadata': {json.dumps(result.metadata)},\")\n",
    "    print(\"    },\")\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (768,) and (0,) not aligned: 768 (dim 0) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mHow should I meditate?\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m results \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49msimilarity_search(query, distance_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdot\u001b[39;49m\u001b[39m'\u001b[39;49m, k\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:350\u001b[0m, in \u001b[0;36mDeepLake.similarity_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    324\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[0;32m    327\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m        List of Documents most similar to the query vector.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch(query\u001b[39m=\u001b[39mquery, k\u001b[39m=\u001b[39mk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:294\u001b[0m, in \u001b[0;36mDeepLake.search\u001b[1;34m(self, query, embedding, k, distance_metric, use_maximal_marginal_relevance, fetch_k, filter, return_score, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m embeddings \u001b[39m=\u001b[39m view\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mnumpy(fetch_chunks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m k_search \u001b[39m=\u001b[39m fetch_k \u001b[39mif\u001b[39;00m use_maximal_marginal_relevance \u001b[39melse\u001b[39;00m k\n\u001b[1;32m--> 294\u001b[0m indices, scores \u001b[39m=\u001b[39m vector_search(\n\u001b[0;32m    295\u001b[0m     query_emb,\n\u001b[0;32m    296\u001b[0m     embeddings,\n\u001b[0;32m    297\u001b[0m     k\u001b[39m=\u001b[39;49mk_search,\n\u001b[0;32m    298\u001b[0m     distance_metric\u001b[39m=\u001b[39;49mdistance_metric\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    299\u001b[0m )\n\u001b[0;32m    301\u001b[0m view \u001b[39m=\u001b[39m view[indices]\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m use_maximal_marginal_relevance:\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:47\u001b[0m, in \u001b[0;36mvector_search\u001b[1;34m(query_embedding, data_vectors, distance_metric, k)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Naive search for nearest neighbors\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m    nearest_indices: List, indices of nearest neighbors\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m# Calculate the distance between the query_vector and all data_vectors\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m distances \u001b[39m=\u001b[39m distance_metric_map[distance_metric](query_embedding, data_vectors)\n\u001b[0;32m     48\u001b[0m nearest_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(distances)\n\u001b[0;32m     50\u001b[0m nearest_indices \u001b[39m=\u001b[39m (\n\u001b[0;32m     51\u001b[0m     nearest_indices[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:k] \u001b[39mif\u001b[39;00m distance_metric \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcos\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m nearest_indices[:k]\n\u001b[0;32m     52\u001b[0m )\n",
      "File \u001b[1;32mf:\\ProgramData\\miniconda3\\envs\\github-qa-langchain\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m maximal_marginal_relevance\n\u001b[0;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     18\u001b[0m distance_metric_map \u001b[39m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a \u001b[39m-\u001b[39m b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39minf),\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcos\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39mdot(a, b\u001b[39m.\u001b[39mT)\n\u001b[0;32m     23\u001b[0m     \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(a) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(b, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m---> 24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m a, b: np\u001b[39m.\u001b[39;49mdot(a, b\u001b[39m.\u001b[39;49mT),\n\u001b[0;32m     25\u001b[0m }\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvector_search\u001b[39m(\n\u001b[0;32m     29\u001b[0m     query_embedding: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     30\u001b[0m     data_vectors: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     31\u001b[0m     distance_metric: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     k: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[0;32m     33\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List, List]:\n\u001b[0;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Naive search for nearest neighbors\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39m    args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m        nearest_indices: List, indices of nearest neighbors\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (768,) and (0,) not aligned: 768 (dim 0) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "How should I meditate?\n",
    "\"\"\"\n",
    "results = db.similarity_search(query, distance_metric='dot', k=4)\n",
    "\n",
    "\n",
    "# print(\"class:\", type(results).__name__)\n",
    "for result in results:\n",
    "    print(f\"Source:  {result.metadata['source']} \")\n",
    "    print(\"\\nContent:  \")\n",
    "    page_content = json.dumps(result.page_content).replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
    "    print(page_content)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify user defined functions using [Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    # if 'com.google' in x['text'].data()['value']:\n",
    "    #     return False\n",
    "    \n",
    "    # filter based on path e.g. extension\n",
    "    metadata =  x['metadata'].data()['value']\n",
    "    return 'scala' in metadata['source'] or 'py' in metadata['source']\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#model = ChatOpenAI(model='gpt-4') # 'gpt-3.5-turbo',\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo') # 'gpt-3.5-turbo',\n",
    "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78407ad8061944abbb6fe41d763e018d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"Use the following pieces of context to answer the users question. \\\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\\\n----------------\\\\nHourly Parameter Definition\\\\nThe parameter &hourly= accepts the following values. Most weather variables are given as an instantaneous value for the indicated hour. Some variables like precipitation are calculated from the preceding hour as an average or sum.\\\\n\\\\nHourly Parameter Definition\\\\nThe parameter &hourly= accepts the following values. Most weather variables are given as an instantaneous value for the indicated hour. Some variables like precipitation are calculated from the preceding hour as an average or sum.\\\\n\\\\ndef _default_scripting_text_mapping(\\\\n    dim: int,\\\\n    vector_field: str = \\\\\"vector_field\\\\\",\\\\n) -> Dict:\\\\n    \\\\\"\\\\\"\\\\\"For Painless Scripting or Script Scoring,the default mapping to create index.\\\\\"\\\\\"\\\\\"\\\\n    return {\\\\n        \\\\\"mappings\\\\\": {\\\\n            \\\\\"properties\\\\\": {\\\\n                vector_field: {\\\\\"type\\\\\": \\\\\"knn_vector\\\\\", \\\\\"dimension\\\\\": dim},\\\\n            }\\\\n        }\\\\n    }\\\\n\\\\ndef _default_scripting_text_mapping(\\\\n    dim: int,\\\\n    vector_field: str = \\\\\"vector_field\\\\\",\\\\n) -> Dict:\\\\n    \\\\\"\\\\\"\\\\\"For Painless Scripting or Script Scoring,the default mapping to create index.\\\\\"\\\\\"\\\\\"\\\\n    return {\\\\n        \\\\\"mappings\\\\\": {\\\\n            \\\\\"properties\\\\\": {\\\\n                vector_field: {\\\\\"type\\\\\": \\\\\"knn_vector\\\\\", \\\\\"dimension\\\\\": dim},\\\\n            }\\\\n        }\\\\n    }\\\\n\\\\ndef _default_script_query(\\\\n    query_vector: List[float],\\\\n    space_type: str = \\\\\"l2\\\\\",\\\\n    pre_filter: Dict = MATCH_ALL_QUERY,\\\\n    vector_field: str = \\\\\"vector_field\\\\\",\\\\n) -> Dict:\\\\n    \\\\\"\\\\\"\\\\\"For Script Scoring Search, this is the default query.\\\\\"\\\\\"\\\\\"\\\\n    return {\\\\n        \\\\\"query\\\\\": {\\\\n            \\\\\"script_score\\\\\": {\\\\n                \\\\\"query\\\\\": pre_filter,\\\\n                \\\\\"script\\\\\": {\\\\n                    \\\\\"source\\\\\": \\\\\"knn_score\\\\\",\\\\n                    \\\\\"lang\\\\\": \\\\\"knn\\\\\",\\\\n                    \\\\\"params\\\\\": {\\\\n                        \\\\\"field\\\\\": vector_field,\\\\n                        \\\\\"query_value\\\\\": query_vector,\\\\n                        \\\\\"space_type\\\\\": space_type,\\\\n                    },\\\\n                },\\\\n            }\\\\n        }\\\\n    }\\\\n\\\\ndef _default_script_query(\\\\n    query_vector: List[float],\\\\n    space_type: str = \\\\\"l2\\\\\",\\\\n    pre_filter: Dict = MATCH_ALL_QUERY,\\\\n    vector_field: str = \\\\\"vector_field\\\\\",\\\\n) -> Dict:\\\\n    \\\\\"\\\\\"\\\\\"For Script Scoring Search, this is the default query.\\\\\"\\\\\"\\\\\"\\\\n    return {\\\\n        \\\\\"query\\\\\": {\\\\n            \\\\\"script_score\\\\\": {\\\\n                \\\\\"query\\\\\": pre_filter,\\\\n                \\\\\"script\\\\\": {\\\\n                    \\\\\"source\\\\\": \\\\\"knn_score\\\\\",\\\\n                    \\\\\"lang\\\\\": \\\\\"knn\\\\\",\\\\n                    \\\\\"params\\\\\": {\\\\n                        \\\\\"field\\\\\": vector_field,\\\\n                        \\\\\"query_value\\\\\": query_vector,\\\\n                        \\\\\"space_type\\\\\": space_type,\\\\n                    },\\\\n                },\\\\n            }\\\\n        }\\\\n    }\\\\n\\\\ndef similarity_search_with_score(\\\\n        self, query: str, k: int = 4\\\\n    ) -> List[Tuple[Document, float]]:\\\\n        \\\\\"\\\\\"\\\\\"Return docs most similar to query.\\\\n\\\\n        Args:\\\\n            query: Text to look up documents similar to.\\\\n            k: Number of Documents to return. Defaults to 4.\\\\n\\\\n        Returns:\\\\n            List of Documents most similar to the query and score for each\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        try:\\\\n            from redis.commands.search.query import Query\\\\n        except ImportError:\\\\n            raise ValueError(\\\\n                \\\\\"Could not import redis python package. \\\\\"\\\\n                \\\\\"Please install it with `pip install redis`.\\\\\"\\\\n            )\\\\n\\\\n        # Creates embedding vector from user query\\\\n        embedding = self.embedding_function(query)\\\\n\\\\ndef similarity_search_with_score(\\\\n        self, query: str, k: int = 4\\\\n    ) -> List[Tuple[Document, float]]:\\\\n        \\\\\"\\\\\"\\\\\"Return docs most similar to query.\\\\n\\\\n        Args:\\\\n            query: Text to look up documents similar to.\\\\n            k: Number of Documents to return. Defaults to 4.\\\\n\\\\n        Returns:\\\\n            List of Documents most similar to the query and score for each\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        try:\\\\n            from redis.commands.search.query import Query\\\\n        except ImportError:\\\\n            raise ValueError(\\\\n                \\\\\"Could not import redis python package. \\\\\"\\\\n                \\\\\"Please install it with `pip install redis`.\\\\\"\\\\n            )\\\\n\\\\n        # Creates embedding vector from user query\\\\n        embedding = self.embedding_function(query)\\\\n\\\\nBy default supports Approximate Search.\\\\n        Also supports Script Scoring and Painless Scripting.\\\\n\\\\n        Args:\\\\n            query: Text to look up documents similar to.\\\\n            k: Number of Documents to return. Defaults to 4.\\\\n\\\\n        Returns:\\\\n            List of Documents most similar to the query.\\\\n\\\\n        Optional Args:\\\\n            vector_field: Document field embeddings are stored in. Defaults to\\\\n            \\\\\"vector_field\\\\\".\\\\n\\\\n            text_field: Document field the text of the document is stored in. Defaults\\\\n            to \\\\\"text\\\\\".\\\\n\\\\n            metadata_field: Document field that metadata is stored in. Defaults to\\\\n            \\\\\"metadata\\\\\".\\\\n            Can be set to a special value \\\\\"*\\\\\" to include the entire document.\\\\n\\\\n        Optional Args for Approximate Search:\\\\n            search_type: \\\\\"approximate_search\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            size: number of results the query actually returns; default: 4\\\\n\\\\nBy default supports Approximate Search.\\\\n        Also supports Script Scoring and Painless Scripting.\\\\n\\\\n        Args:\\\\n            query: Text to look up documents similar to.\\\\n            k: Number of Documents to return. Defaults to 4.\\\\n\\\\n        Returns:\\\\n            List of Documents most similar to the query.\\\\n\\\\n        Optional Args:\\\\n            vector_field: Document field embeddings are stored in. Defaults to\\\\n            \\\\\"vector_field\\\\\".\\\\n\\\\n            text_field: Document field the text of the document is stored in. Defaults\\\\n            to \\\\\"text\\\\\".\\\\n\\\\n            metadata_field: Document field that metadata is stored in. Defaults to\\\\n            \\\\\"metadata\\\\\".\\\\n            Can be set to a special value \\\\\"*\\\\\" to include the entire document.\\\\n\\\\n        Optional Args for Approximate Search:\\\\n            search_type: \\\\\"approximate_search\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            size: number of results the query actually returns; default: 4\\\\n\\\\ndef print_text(text: str, color: Optional[str] = None, end: str = \\\\\"\\\\\") -> None:\\\\n    \\\\\"\\\\\"\\\\\"Print text with highlighting and no end characters.\\\\\"\\\\\"\\\\\"\\\\n    if color is None:\\\\n        text_to_print = text\\\\n    else:\\\\n        text_to_print = get_colored_text(text, color)\\\\n    print(text_to_print, end=end)\\\\n\\\\ndef print_text(text: str, color: Optional[str] = None, end: str = \\\\\"\\\\\") -> None:\\\\n    \\\\\"\\\\\"\\\\\"Print text with highlighting and no end characters.\\\\\"\\\\\"\\\\\"\\\\n    if color is None:\\\\n        text_to_print = text\\\\n    else:\\\\n        text_to_print = get_colored_text(text, color)\\\\n    print(text_to_print, end=end)\\\\n\\\\nOptional Args for Script Scoring Search:\\\\n            search_type: \\\\\"script_scoring\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            space_type: \\\\\"l2\\\\\", \\\\\"l1\\\\\", \\\\\"linf\\\\\", \\\\\"cosinesimil\\\\\", \\\\\"innerproduct\\\\\",\\\\n            \\\\\"hammingbit\\\\\"; default: \\\\\"l2\\\\\"\\\\n\\\\n            pre_filter: script_score query to pre-filter documents before identifying\\\\n            nearest neighbors; default: {\\\\\"match_all\\\\\": {}}\\\\n\\\\n        Optional Args for Painless Scripting Search:\\\\n            search_type: \\\\\"painless_scripting\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            space_type: \\\\\"l2Squared\\\\\", \\\\\"l1Norm\\\\\", \\\\\"cosineSimilarity\\\\\"; default: \\\\\"l2Squared\\\\\"\\\\n\\\\nOptional Args for Script Scoring Search:\\\\n            search_type: \\\\\"script_scoring\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            space_type: \\\\\"l2\\\\\", \\\\\"l1\\\\\", \\\\\"linf\\\\\", \\\\\"cosinesimil\\\\\", \\\\\"innerproduct\\\\\",\\\\n            \\\\\"hammingbit\\\\\"; default: \\\\\"l2\\\\\"\\\\n\\\\n            pre_filter: script_score query to pre-filter documents before identifying\\\\n            nearest neighbors; default: {\\\\\"match_all\\\\\": {}}\\\\n\\\\n        Optional Args for Painless Scripting Search:\\\\n            search_type: \\\\\"painless_scripting\\\\\"; default: \\\\\"approximate_search\\\\\"\\\\n\\\\n            space_type: \\\\\"l2Squared\\\\\", \\\\\"l1Norm\\\\\", \\\\\"cosineSimilarity\\\\\"; default: \\\\\"l2Squared\\\\\"\\\\n\\\\nReturns:\\\\n            bool: Whether or not the drop was successful.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        redis_url = get_from_dict_or_env(kwargs, \\\\\"redis_url\\\\\", \\\\\"REDIS_URL\\\\\")\\\\n        try:\\\\n            import redis\\\\n        except ImportError:\\\\n            raise ValueError(\\\\n                \\\\\"Could not import redis python package. \\\\\"\\\\n                \\\\\"Please install it with `pip install redis`.\\\\\"\\\\n            )\\\\n        try:\\\\n            # We need to first remove redis_url from kwargs,\\\\n            # otherwise passing it to Redis will result in an error.\\\\n            kwargs.pop(\\\\\"redis_url\\\\\")\\\\n            client = redis.from_url(url=redis_url, **kwargs)\\\\n        except ValueError as e:\\\\n            raise ValueError(f\\\\\"Your redis connected error: {e}\\\\\")\\\\n        # Check if index exists\\\\n        try:\\\\n            client.ft(index_name).dropindex(delete_documents)\\\\n            logger.info(\\\\\"Drop index\\\\\")\\\\n            return True\\\\n        except:  # noqa: E722\\\\n            # Index not exist\\\\n            return False\\\\n\\\\nReturns:\\\\n            bool: Whether or not the drop was successful.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        redis_url = get_from_dict_or_env(kwargs, \\\\\"redis_url\\\\\", \\\\\"REDIS_URL\\\\\")\\\\n        try:\\\\n            import redis\\\\n        except ImportError:\\\\n            raise ValueError(\\\\n                \\\\\"Could not import redis python package. \\\\\"\\\\n                \\\\\"Please install it with `pip install redis`.\\\\\"\\\\n            )\\\\n        try:\\\\n            # We need to first remove redis_url from kwargs,\\\\n            # otherwise passing it to Redis will result in an error.\\\\n            kwargs.pop(\\\\\"redis_url\\\\\")\\\\n            client = redis.from_url(url=redis_url, **kwargs)\\\\n        except ValueError as e:\\\\n            raise ValueError(f\\\\\"Your redis connected error: {e}\\\\\")\\\\n        # Check if index exists\\\\n        try:\\\\n            client.ft(index_name).dropindex(delete_documents)\\\\n            logger.info(\\\\\"Drop index\\\\\")\\\\n            return True\\\\n        except:  # noqa: E722\\\\n            # Index not exist\\\\n            return False\\\\n\\\\ndef _get_driver(self) -> Union[\\\\\"Chrome\\\\\", \\\\\"Firefox\\\\\"]:\\\\n        \\\\\"\\\\\"\\\\\"Create and return a WebDriver instance based on the specified browser.\\\\n\\\\n        Raises:\\\\n            ValueError: If an invalid browser is specified.\\\\n\\\\n        Returns:\\\\n            Union[Chrome, Firefox]: A WebDriver instance for the specified browser.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if self.browser.lower() == \\\\\"chrome\\\\\":\\\\n            from selenium.webdriver import Chrome\\\\n            from selenium.webdriver.chrome.options import Options as ChromeOptions\\\\n\\\\ndef _get_driver(self) -> Union[\\\\\"Chrome\\\\\", \\\\\"Firefox\\\\\"]:\\\\n        \\\\\"\\\\\"\\\\\"Create and return a WebDriver instance based on the specified browser.\\\\n\\\\n        Raises:\\\\n            ValueError: If an invalid browser is specified.\\\\n\\\\n        Returns:\\\\n            Union[Chrome, Firefox]: A WebDriver instance for the specified browser.\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if self.browser.lower() == \\\\\"chrome\\\\\":\\\\n            from selenium.webdriver import Chrome\\\\n            from selenium.webdriver.chrome.options import Options as ChromeOptions\\\\n\\\\nelse:\\\\n            toret = \\\\\"No good search result found\\\\\"\\\\n        return toret\\\\n\\\\nelse:\\\\n            toret = \\\\\"No good search result found\\\\\"\\\\n        return toret\"}, {\"role\": \"user\", \"content\": \"How do I load a 4bit quantized LLaMA model into langchain for inference tasks? The model was quantized using GPTQ, and has a group size of 128?\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.7}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4838 request_id=3111b5c1c9fe6d25c9d28525dc37bd8f response_code=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: How do I load a 4bit quantized LLaMA model into langchain for inference tasks? The model was quantized using GPTQ, and has a group size of 128? \n",
      "\n",
      "**Answer**: I'm sorry, but I am not familiar with the Langchain framework or with the GPTQ quantization method. Without more information, I cannot provide an accurate answer to your question. Can you provide more details or context about what you are trying to do? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How do I load a 4bit quantized LLaMA model into langchain for inference tasks? The model was quantized using GPTQ, and has a group size of 128?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "# for question in questions:  \n",
    "#     result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "#     print(result)\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
